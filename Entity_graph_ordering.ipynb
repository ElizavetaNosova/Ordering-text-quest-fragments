{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Entity graph ordering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FEDwono-32L"
      },
      "source": [
        "import networkx as nx\n",
        "from networkx.readwrite import json_graph\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import json\n",
        "import itertools\n",
        "import torch\n",
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "from copy import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCCiPs0y_a6U",
        "outputId": "03192936-3543-4e17-96d3-ee44447fd40d"
      },
      "source": [
        "#!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 08:27:16--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ru.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4496459151 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.ru.300.bin.gz.1’\n",
            "\n",
            "cc.ru.300.bin.gz.1    0%[                    ]   3.63M  2.77MB/s               ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JVXwcoJKcNm",
        "outputId": "feee01cc-bf0c-407f-fc3a-b6c7460c27a5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-DgIT_TKe-U"
      },
      "source": [
        "os.chdir('gdrive/MyDrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J80JvmeI_ZQg"
      },
      "source": [
        "fasttext_model = FastText.load_fasttext_format('cc.ru.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbabcgIF-32P"
      },
      "source": [
        "def choose_tag(fragment_noun_tags):\n",
        "    if 'subj' in fragment_noun_tags:\n",
        "        return subj\n",
        "    elif 'obj' in fragment_noun_tags:\n",
        "        return 'obj'\n",
        "    else:\n",
        "        return 'other_dep'\n",
        "\n",
        "def quest_path2entity_graph(path, morphograph, morphology_key='node_morphodata', text_key='fragment_text'):\n",
        "    G = nx.Graph()\n",
        "    \n",
        "    noun_lemmas_counter = Counter()\n",
        "    nodes = morphograph.nodes()\n",
        "    \n",
        "    node2chosen_syntactic_tags = {}\n",
        "    nouns2nodes = defaultdict(set)\n",
        "    \n",
        "    for node in path:\n",
        "        current_node2all_syntactic_tags = defaultdict(set)\n",
        "        for noun_data in nodes[node][morphology_key]['nouns']:\n",
        "            noun_lemma = noun_data['lemma']\n",
        "            noun_lemmas_counter.update([noun_lemma])\n",
        "            nouns2nodes['noun_'+noun_lemma].add(node)\n",
        "            dep_type = 'other_dep'\n",
        "            dependency = noun_data['deprel']\n",
        "            if 'subj' in dependency:\n",
        "                dep_type = 'subject'\n",
        "            elif 'obj' in dependency:\n",
        "                dep_type = 'object'\n",
        "            current_node2all_syntactic_tags[noun_lemma].add(dep_type)\n",
        "        \n",
        "        node2chosen_syntactic_tags[node] = {noun:choose_tag(current_node2all_syntactic_tags[noun]) for noun in current_node2all_syntactic_tags}\n",
        "        \n",
        "    relevant_nouns = [noun for noun in noun_lemmas_counter if noun_lemmas_counter[noun]>1]\n",
        "    G.add_node('global', fragment_text='UNK')\n",
        "    for relevant_noun in relevant_nouns:\n",
        "        G.add_node('noun_'+relevant_noun, fragment_text=relevant_noun)\n",
        "    for node in path:\n",
        "        G.add_node(node, fragment_text=nodes[node][text_key])\n",
        "        G.add_edge(node, 'global', label='global')\n",
        "        for noun, syntactic_role in node2chosen_syntactic_tags[node].items():\n",
        "            if noun in relevant_nouns:\n",
        "                G.add_edge(node, 'noun_'+noun, label=syntactic_role)\n",
        "    for noun in nouns2nodes:\n",
        "        entity_neighbours = itertools.combinations(nouns2nodes[noun], 2)\n",
        "        for pair in entity_neighbours:\n",
        "            G.add_edge(*pair, label='fragments_pair')\n",
        "    return G, [['noun_'+relevant_noun for noun in relevant_nouns]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FENZGTMY-32S"
      },
      "source": [
        "BOOK_GRAPHS_DIRECTORY = 'D:\\Диплом_текстовые_квесты\\Data\\quest_books_graphs_morphology'\n",
        "ONLINE_GRAPHS_DIRECTORY = 'D:\\Диплом_текстовые_квесты\\Data\\Questbook_online_grammar'\n",
        "\n",
        "dev_online_graphs, test_online_graphs = train_test_split(os.listdir(ONLINE_GRAPHS_DIRECTORY), random_state=42, test_size=0.4)\n",
        "\n",
        "all_dev_graphs = [os.path.join(BOOK_GRAPHS_DIRECTORY, book_graph) for book_graph in os.listdir(BOOK_GRAPHS_DIRECTORY)] + [os.path.join(ONLINE_GRAPHS_DIRECTORY, online_graph) for online_graph in dev_online_graphs]\n",
        "test_graphs_paths = [os.path.join(ONLINE_GRAPHS_DIRECTORY, test_graph) for test_graph in test_online_graphs]\n",
        "\n",
        "train_paths, valid_paths = train_test_split(all_dev_graphs, random_state=42, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfHUnGnr-32T"
      },
      "source": [
        "def correct_path(path, graph, morphodata_field, text_field='fragment_text'):\n",
        "    nodes = graph.nodes()\n",
        "    correct_path = [fragment_id for fragment_id in path if  morphodata_field in nodes[fragment_id] and text_field in nodes[fragment_id] and isinstance(nodes[fragment_id][text_field], str)]\n",
        "    return correct_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0D0Ti13AJKj"
      },
      "source": [
        "COLAB_TRAIN_DATA_DIRECTORY = 'tokenized_ordering_train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh01aFRlHtPP"
      },
      "source": [
        "COLAB_TEST_DATA_DIRECTORY = 'tokenized_ordering_test_joined'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWZqIecw-rAu"
      },
      "source": [
        "edge_labels = [None, 'global', 'fragments_pair','subject', 'object', 'other_dep']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUKNaK_X-32X"
      },
      "source": [
        "class GraphFastTextPreprocessor(torch.nn.Module):\n",
        "    def __init__(self, fasttext_model, edge_labels, fasttext_embedding_dim=300, lstm_hidden_size=50):\n",
        "        super().__init__()\n",
        "        self.edges_vocab = {label:idx for idx, label in enumerate(edge_labels)}\n",
        "        self.fasttext_model = fasttext_model\n",
        "        self.fasttext_embedding_dim = fasttext_embedding_dim\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.output_size = lstm_hidden_size*4\n",
        "        self.encoding_lstm = torch.nn.LSTM(input_size=fasttext_embedding_dim, hidden_size=lstm_hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.non_linear = torch.sigmoid\n",
        "        \n",
        "    \n",
        "    def forward(self, graph, quest_correct_path, quest_nouns):\n",
        "        nodes_order = ['global'] + quest_correct_path + quest_nouns\n",
        "        node2id = {node:idx for idx, node in enumerate(nodes_order)}\n",
        "        nodes = graph.nodes()\n",
        "        edges = graph.edges()\n",
        "        graph_transition_table = torch.zeros(len(nodes_order), len(nodes_order)).to(device)\n",
        "        for node1, node2 in edges:\n",
        "            node1_idx = node2id[node1]\n",
        "            node2_idx = node2id[node2]\n",
        "            label_idx = self.edges_vocab[edges[(node1, node2)]['label']]\n",
        "            graph_transition_table[node1_idx][node2_idx] = label_idx\n",
        "            graph_transition_table[node2_idx][node1_idx] = label_idx\n",
        "        texts = []\n",
        "        for node in nodes_order:\n",
        "            if 'tokenizen_text' in nodes[node]:\n",
        "                texts.append([nodes[node]['tokenized_text']])\n",
        "            elif 'fragment_text' in nodes[node]:\n",
        "                texts.append(nodes[node]['fragment_text'])\n",
        "        text_embedded_nodes = self.embed_text_batch(texts).to(device)\n",
        "        text_encoded_nodes = self.lstm_encode(text_embedded_nodes)\n",
        "        #global nodes is initialized with zeros\n",
        "        text_encoded_nodes = torch.cat((torch.zeros(1, text_encoded_nodes.shape[1]).to(device), text_encoded_nodes[1:].clone()), 0)\n",
        "        #text_encoded_nodes[0] = text_encoded_nodes[0].clone() * 0\n",
        "        return text_encoded_nodes, graph_transition_table.int()\n",
        "                   \n",
        "    def embed_text_batch(self, tokenized_texts):\n",
        "        embedded_batch = []\n",
        "        for tokenized_text in tokenized_texts:\n",
        "            embeddings = [self.fasttext_model[token] for token in tokenized_text if token in self.fasttext_model]\n",
        "            embedded_batch.append(embeddings)\n",
        "        max_len = max([len(embeddings) for embeddings in embedded_batch])\n",
        "        padded_batch = []\n",
        "        for embeddings in embedded_batch:\n",
        "            padded_embeddings = [np.zeros(self.fasttext_embedding_dim) for i in range(max_len-len(embeddings))] + embeddings\n",
        "            padded_batch.append(padded_embeddings)\n",
        "        return torch.tensor(padded_batch).float()\n",
        "        \n",
        "    def lstm_encode(self, embeddings):\n",
        "        _, representation = self.encoding_lstm(embeddings)\n",
        "        representation_vector = torch.cat((representation[0].clone()[0].clone(), representation[0].clone()[1].clone(), representation[1].clone()[0].clone(), representation[1].clone()[1].clone()),1)\n",
        "        return self.non_linear(representation_vector)\n",
        "        #return representation\n",
        "        #return torch.cat((representation[0][0], representation[0][1], representation[1][0], representation[1][1]),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tcUJZ2L-32Y"
      },
      "source": [
        "class GRN(torch.nn.Module):\n",
        "    def __init__(self, num_edge_embeddings, edge_embedding_dim, edge_pad_id=0, num_iterations=2):\n",
        "        super().__init__()\n",
        "        self.num_iterations=num_iterations\n",
        "        self.edge_embedding_layer = torch.nn.Embedding(num_embeddings=num_edge_embeddings, embedding_dim=1, padding_idx=edge_pad_id)\n",
        "        \n",
        "        self.forgetting_linear = torch.nn.Linear(edge_embedding_dim, edge_embedding_dim)\n",
        "        self.forgetting_nonlinear = torch.sigmoid\n",
        "\n",
        "        self.incoming_linear = torch.nn.Linear(edge_embedding_dim, edge_embedding_dim)\n",
        "        self.incoming_nonlinear = torch.tanh\n",
        "\n",
        "        self.aggregation_nonlinear = torch.tanh\n",
        "\n",
        "    def forward(self, nodes_embeddings, transition_tables):\n",
        "        embedded_transitions = torch.squeeze(self.edge_embedding_layer(transition_tables)).float()\n",
        "        for i in range(self.num_iterations):\n",
        "            incoming_sygnal = torch.matmul(embedded_transitions, nodes_embeddings)\n",
        "            incoming_sygnal = self.incoming_nonlinear(self.incoming_linear(incoming_sygnal))\n",
        "            sygnal2forget = self.forgetting_linear(self.forgetting_nonlinear(nodes_embeddings))\n",
        "            nodes_embeddings = self.aggregation_nonlinear(nodes_embeddings - sygnal2forget + incoming_sygnal)\n",
        "        return nodes_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBADhqiTvzrG"
      },
      "source": [
        "class OrderingNetwork(torch.nn.Module):\n",
        "    def __init__(self, fasttext_model, edge_labels, fasttext_embedding_dim=300, encoder_lstm_hidden_size=50, pointer_network_lstm_hidden_size=50, pointer_network_hidden_dim=25):\n",
        "        super().__init__()\n",
        "        self.graph_preprocessor = GraphFastTextPreprocessor(fasttext_model, edge_labels, fasttext_embedding_dim, encoder_lstm_hidden_size)\n",
        "        self.text_embedding_dim = self.graph_preprocessor.output_size\n",
        "        self.GRN = GRN(num_edge_embeddings=len(edge_labels), edge_embedding_dim=self.text_embedding_dim)\n",
        "        self.pointer_network_lstm_hidden_size = pointer_network_lstm_hidden_size\n",
        "        self.pointer_network_hidden_dim = pointer_network_hidden_dim\n",
        "\n",
        "        self.history_lstm = torch.nn.LSTM(input_size=self.text_embedding_dim, hidden_size=self.text_embedding_dim, batch_first=True)\n",
        "        self.history_linear = torch.nn.Linear(self.text_embedding_dim, self.pointer_network_hidden_dim)\n",
        "        self.candidate_linear = torch.nn.Linear(self.text_embedding_dim, self.pointer_network_hidden_dim)\n",
        "        self.scoring_linear = torch.nn.Linear(self.pointer_network_hidden_dim, 1)\n",
        "  \n",
        "\n",
        "    def forward(self, graph, quest_correct_path, quest_nouns):\n",
        "        text_encoded_nodes, graph_transition_table = self.graph_preprocessor(graph, quest_correct_path, quest_nouns)\n",
        "        text_encoded_nodes = self.GRN(text_encoded_nodes, graph_transition_table)\n",
        "\n",
        "        #first vector is the global state representation; the next vectors represent fragment nodes\n",
        "        #during training I will treat previous nodes as possible candidates, repetitions can be prohibited apostpriori\n",
        "        correct_history = text_encoded_nodes[:len(quest_correct_path)].clone()\n",
        "        candidates = text_encoded_nodes[1:len(quest_correct_path)+1].clone()\n",
        "\n",
        "        candidates_representation = torch.tanh(self.candidate_linear(candidates))\n",
        "        history_representation = torch.tanh(self.history_linear(correct_history))\n",
        "        scores = torch.zeros(len(candidates), len(candidates)).to(device)\n",
        "        for step_idx in range(history_representation.shape[0]):\n",
        "            history_step = history_representation[step_idx, :].clone()\n",
        "            scores[step_idx] =  scores[step_idx].clone() + torch.squeeze(torch.sigmoid(self.scoring_linear(candidates_representation+history_step)))\n",
        "       # return torch.softmax(scores, 1)\n",
        "        return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrPH7J9h4sbf"
      },
      "source": [
        "ordering_model = OrderingNetwork(fasttext_model, edge_labels)\n",
        "ordering_model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbOTFD_am6-O"
      },
      "source": [
        "optimizer =  torch.optim.Adam([{'params': ordering_model.parameters()}], \n",
        "                               lr = 1e-3)\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixFDWiz8iPbS"
      },
      "source": [
        "ordering_model.load_state_dict(torch.load('graph_based_ordering_model.pth'))\n",
        "optimizer.load_state_dict(torch.load('graph_based_ordering_model_optimizer.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ9fLDVvj9bR"
      },
      "source": [
        "text_encoded_nodes_GRN = ordering_model.GRN(text_encoded_nodes, graph_transition_table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74eEipyBo6MT"
      },
      "source": [
        "device='cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRTfNAqLpCwU"
      },
      "source": [
        "train_paths = os.listdir(COLAB_TRAIN_DATA_DIRECTORY)\n",
        "real_i = 5001\n",
        "for i in range(5001, len(train_paths)):\n",
        "    prepared_path_file = train_paths[i]\n",
        "    \n",
        "    path_path = os.path.join(COLAB_TRAIN_DATA_DIRECTORY, prepared_path_file)\n",
        "    path_data = json.load(open(path_path, encoding='utf-8'))\n",
        "    path_data[2] = json_graph.node_link_graph(path_data[2])\n",
        "    path_data[3] = [node for node in path_data[2].nodes() if isinstance(node, str) and 'noun_' in node]\n",
        "    try:\n",
        "        _, correct_path, graph, nouns = path_data\n",
        "        if len(nouns) < 200 and len(correct_path) + len(nouns) < 220:\n",
        "            print('num_fragments', len(correct_path), 'num_nouns', len(nouns))\n",
        "            scores = ordering_model(graph, correct_path, nouns)\n",
        "            correct_scores = torch.eye(scores.shape[0]).to(device)\n",
        "\n",
        "            loss = criterion(scores, correct_scores)\n",
        "            loss.backward()\n",
        "            real_i += 1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "    if real_i % 10 == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    if real_i % 50 == 0:\n",
        "       torch.save(ordering_model.state_dict(), 'graph_based_ordering_model.pth')\n",
        "       torch.save(optimizer.state_dict(), 'graph_based_ordering_model_optimizer.pth')\n",
        "       print(real_i, i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpByxSFIIRuz"
      },
      "source": [
        "def entity_graph_beam_search(graph, path, nouns, num_candidates = 3):\n",
        "    text_encoded_nodes, graph_transition_table = ordering_model.graph_preprocessor(graph, path, nouns)\n",
        "    text_encoded_nodes = ordering_model.GRN(text_encoded_nodes, graph_transition_table)\n",
        "\n",
        "    candidates = text_encoded_nodes[1:len(path)+1].clone()\n",
        "    initial_history = torch.unsqueeze(text_encoded_nodes[0].clone(), 0)\n",
        "    unordered_fragments_representation = torch.tanh(ordering_model.candidate_linear(candidates))\n",
        "    order_candidates = [{'order': [], 'history': initial_history, 'probability': 1}]\n",
        "    for i in range(len(candidates)):\n",
        "        next_step_order_candidates = []\n",
        "        for order_candidate in order_candidates:\n",
        "            \n",
        "            history_representation = ordering_model.history_lstm(torch.unsqueeze(order_candidate['history'], 0))[0] \n",
        "            current_history_step = torch.squeeze(torch.tanh(ordering_model.history_linear(history_representation[:,-1,:])))\n",
        "\n",
        "  \n",
        "            probabilities = torch.tensor([torch.sigmoid(ordering_model.scoring_linear(current_history_step + fragment_representation)) if i not in order_candidate['order'] else 0 for i, fragment_representation in enumerate(unordered_fragments_representation)])\n",
        "            current_step_num_candidates = min(num_candidates, len(candidates)-len(order_candidate['order']))\n",
        "            next_idxs = torch.argsort(probabilities,  descending=True)[:current_step_num_candidates]\n",
        "            for next_idx in next_idxs:\n",
        "                probability = float(probabilities[next_idx])\n",
        "                next_step_order = copy(order_candidate)\n",
        "                next_step_order['probability'] = next_step_order['probability'] * probability\n",
        "                next_step_order['order'] = next_step_order['order'] + [int(next_idx)]\n",
        "                next_step_order['history'] = torch.cat((next_step_order['history'], torch.unsqueeze(candidates[next_idx], 0)), 0)\n",
        "                next_step_order_candidates.append(next_step_order)\n",
        "        order_candidates = sorted(next_step_order_candidates, key = lambda x: ['probability'])[:num_candidates]\n",
        "    return order_candidates[0]['order']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5V16voPoz0v"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVyJ6lXwovzK"
      },
      "source": [
        "def scoring_entity_graph_beam_search(graph, path, nouns, num_candidates = 3, epsilon=10**(-9)):\n",
        "    text_encoded_nodes, graph_transition_table = ordering_model.graph_preprocessor(graph, path, nouns)\n",
        "    text_encoded_nodes = ordering_model.GRN(text_encoded_nodes, graph_transition_table)\n",
        "\n",
        "    candidates = text_encoded_nodes[1:len(path)+1].clone()\n",
        "    initial_history = torch.unsqueeze(text_encoded_nodes[0].clone(), 0)\n",
        "    unordered_fragments_representation = torch.tanh(ordering_model.candidate_linear(candidates))\n",
        "    order_candidates = [{'order': [], 'history': initial_history, 'probability': 0}]\n",
        "    for i in range(len(candidates)):\n",
        "        next_step_order_candidates = []\n",
        "        for order_candidate in order_candidates:\n",
        "            \n",
        "            history_representation = ordering_model.history_lstm(torch.unsqueeze(order_candidate['history'], 0))[0] \n",
        "            current_history_step = torch.squeeze(torch.tanh(ordering_model.history_linear(history_representation[:,-1,:])))\n",
        "\n",
        "  \n",
        "            probabilities = torch.tensor([torch.sigmoid(ordering_model.scoring_linear(current_history_step + fragment_representation)) if i not in order_candidate['order'] else 0 for i, fragment_representation in enumerate(unordered_fragments_representation)])\n",
        "            current_step_num_candidates = min(num_candidates, len(candidates)-len(order_candidate['order']))\n",
        "            next_idxs = torch.argsort(probabilities,  descending=True)[:current_step_num_candidates]\n",
        "            for next_idx in next_idxs:\n",
        "                score = float(probabilities[next_idx]) if float(probabilities[next_idx]) > 0 else epsilon\n",
        "                next_step_order = copy(order_candidate)\n",
        "                next_step_order['probability'] = next_step_order['probability'] + math.log(score)\n",
        "                next_step_order['order'] = next_step_order['order'] + [int(next_idx)]\n",
        "                next_step_order['history'] = torch.cat((next_step_order['history'], torch.unsqueeze(candidates[next_idx], 0)), 0)\n",
        "                next_step_order_candidates.append(next_step_order)\n",
        "        order_candidates = sorted(next_step_order_candidates, key = lambda x: ['probability'])[:num_candidates]\n",
        "    return {'order':order_candidates[0]['order'], 'score':order_candidates[0]['probability']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-5shlgy3McA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "659cf6ed-c143-4b43-ee63-d6d6bfebf9cb"
      },
      "source": [
        "ordering_model = OrderingNetwork(fasttext_model, edge_labels)\n",
        "ordering_model.load_state_dict(torch.load('graph_based_ordering_model.pth'))\n",
        "ordering_model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderingNetwork(\n",
              "  (graph_preprocessor): GraphFastTextPreprocessor(\n",
              "    (encoding_lstm): LSTM(300, 50, batch_first=True, bidirectional=True)\n",
              "  )\n",
              "  (GRN): GRN(\n",
              "    (edge_embedding_layer): Embedding(6, 1, padding_idx=0)\n",
              "    (forgetting_linear): Linear(in_features=200, out_features=200, bias=True)\n",
              "    (incoming_linear): Linear(in_features=200, out_features=200, bias=True)\n",
              "  )\n",
              "  (history_lstm): LSTM(200, 200, batch_first=True)\n",
              "  (history_linear): Linear(in_features=200, out_features=25, bias=True)\n",
              "  (candidate_linear): Linear(in_features=200, out_features=25, bias=True)\n",
              "  (scoring_linear): Linear(in_features=25, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCj02bLjP2S-"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEywzuw-Iri5"
      },
      "source": [
        "predictions = []\n",
        "for prepared_path_file in tqdm(os.listdir(COLAB_TEST_DATA_DIRECTORY)):\n",
        "     \n",
        "    try:\n",
        "        with open(os.path.join(COLAB_TEST_DATA_DIRECTORY, file)):\n",
        "            path_path = os.path.join(COLAB_TEST_DATA_DIRECTORY, prepared_path_file)\n",
        "            path_data = json.load(open(path_path, encoding='utf-8'))\n",
        "            path_data[2] = json_graph.node_link_graph(path_data[2])\n",
        "            path_data[3] = [node for node in path_data[2].nodes() if isinstance(node, str) and 'noun_' in node]\n",
        "\n",
        "            with torch.no_grad():\n",
        "\n",
        "                _, path, graph, nouns = path_data\n",
        "                if len(path) <= 30 and len(path)+len(nouns) <= 120:\n",
        "                    prediction = entity_graph_beam_search(graph, path, nouns)\n",
        "                    predictions.append(prediction)\n",
        "\n",
        "                    if len(predictions) % 100 == 0:\n",
        "                        with open('entity_graph_predictions.json', 'w') as f:\n",
        "                            json.dump(predictions, f)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "with open('entity_graph_predictions.json', 'w') as f:\n",
        "    json.dump(predictions, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJASC-rE2Kk7"
      },
      "source": [
        "with open('entity_graph_predictions.json') as f:\n",
        "    predictions = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoMFvcZv2QAI"
      },
      "source": [
        "from scipy.stats import kendalltau\n",
        "import pandas as pd\n",
        "def longest_correct_subsequence(predicted, correct):\n",
        "    correct_transitions = set([(item_from, item_to) for item_from, item_to in zip(correct[:-1], correct[1:])])\n",
        "    predicted_transitions = [(item_from, item_to) for item_from, item_to in zip(predicted[:-1], predicted[1:])]\n",
        "    predicted_transitions_are_correct = [transition in correct_transitions for transition in predicted_transitions]\n",
        "    \n",
        "    longest_correct_transitions_subsequence = 0\n",
        "    current_correct_transitions_subsequence = 0\n",
        "    #Add False as last item to include last real item checking into the loop\n",
        "    for predicted_transition_is_correct in predicted_transitions_are_correct + [False]:\n",
        "        if predicted_transition_is_correct:\n",
        "            current_correct_transitions_subsequence += 1\n",
        "        else:\n",
        "            if current_correct_transitions_subsequence > longest_correct_transitions_subsequence:\n",
        "                longest_correct_transitions_subsequence = current_correct_transitions_subsequence\n",
        "            current_correct_transitions_subsequence = 0\n",
        "    #return number of items in longest correct sequence (not number of transitions)\n",
        "    return longest_correct_transitions_subsequence + 1 if longest_correct_transitions_subsequence else 0  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U5aFfyT5W36"
      },
      "source": [
        "entity_graph_df = pd.DataFrame(columns = [\"sequence length\", \"Kendall's  tau\", \"Longest correct subsequence\"])\n",
        "for prediction in predictions:\n",
        "    correct = list(range(len(prediction)))\n",
        "    tau = kendalltau(prediction, correct).correlation\n",
        "    lcs = longest_correct_subsequence(prediction, correct)\n",
        "    entity_graph_df.loc[len(entity_graph_df)] = [len(correct), tau, lcs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "id": "B-6Xb2mZ50pH",
        "outputId": "64962496-6c3c-4c58-c4b8-e11ace4172f3"
      },
      "source": [
        "entity_graph_df_aggr = entity_graph_df[entity_graph_df[\"sequence length\"]<=30].groupby(\"sequence length\").describe()[[(\"Kendall's  tau\", 'count'),  (\"Kendall's  tau\",  'mean'), ('Longest correct subsequence',  'mean')]]\n",
        "entity_graph_df_aggr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Kendall's  tau</th>\n",
              "      <th>Longest correct subsequence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>mean</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sequence length</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>100.0</td>\n",
              "      <td>-0.133333</td>\n",
              "      <td>1.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>97.0</td>\n",
              "      <td>-0.088660</td>\n",
              "      <td>1.329897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.0</th>\n",
              "      <td>150.0</td>\n",
              "      <td>0.032889</td>\n",
              "      <td>1.406667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.0</th>\n",
              "      <td>152.0</td>\n",
              "      <td>0.187343</td>\n",
              "      <td>1.421053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.0</th>\n",
              "      <td>125.0</td>\n",
              "      <td>0.073714</td>\n",
              "      <td>1.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.0</th>\n",
              "      <td>189.0</td>\n",
              "      <td>0.124927</td>\n",
              "      <td>1.476190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.0</th>\n",
              "      <td>163.0</td>\n",
              "      <td>-0.039400</td>\n",
              "      <td>1.515337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11.0</th>\n",
              "      <td>222.0</td>\n",
              "      <td>0.114824</td>\n",
              "      <td>1.477477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12.0</th>\n",
              "      <td>212.0</td>\n",
              "      <td>0.113922</td>\n",
              "      <td>1.584906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13.0</th>\n",
              "      <td>174.0</td>\n",
              "      <td>0.049366</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14.0</th>\n",
              "      <td>118.0</td>\n",
              "      <td>0.057739</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.0</th>\n",
              "      <td>85.0</td>\n",
              "      <td>0.123585</td>\n",
              "      <td>1.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16.0</th>\n",
              "      <td>73.0</td>\n",
              "      <td>0.113242</td>\n",
              "      <td>1.369863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17.0</th>\n",
              "      <td>51.0</td>\n",
              "      <td>0.036044</td>\n",
              "      <td>1.431373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18.0</th>\n",
              "      <td>66.0</td>\n",
              "      <td>0.039612</td>\n",
              "      <td>1.378788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19.0</th>\n",
              "      <td>45.0</td>\n",
              "      <td>0.063808</td>\n",
              "      <td>1.533333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.0</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>1.419355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21.0</th>\n",
              "      <td>31.0</td>\n",
              "      <td>-0.107834</td>\n",
              "      <td>1.741935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22.0</th>\n",
              "      <td>27.0</td>\n",
              "      <td>-0.038961</td>\n",
              "      <td>1.518519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.037945</td>\n",
              "      <td>1.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24.0</th>\n",
              "      <td>14.0</td>\n",
              "      <td>-0.010870</td>\n",
              "      <td>1.428571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.060667</td>\n",
              "      <td>1.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26.0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.015385</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27.0</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.205445</td>\n",
              "      <td>1.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28.0</th>\n",
              "      <td>10.0</td>\n",
              "      <td>-0.062434</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29.0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>-0.008484</td>\n",
              "      <td>1.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30.0</th>\n",
              "      <td>12.0</td>\n",
              "      <td>-0.039847</td>\n",
              "      <td>1.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Kendall's  tau           Longest correct subsequence\n",
              "                         count      mean                        mean\n",
              "sequence length                                                     \n",
              "4.0                      100.0 -0.133333                    1.300000\n",
              "5.0                       97.0 -0.088660                    1.329897\n",
              "6.0                      150.0  0.032889                    1.406667\n",
              "7.0                      152.0  0.187343                    1.421053\n",
              "8.0                      125.0  0.073714                    1.576000\n",
              "9.0                      189.0  0.124927                    1.476190\n",
              "10.0                     163.0 -0.039400                    1.515337\n",
              "11.0                     222.0  0.114824                    1.477477\n",
              "12.0                     212.0  0.113922                    1.584906\n",
              "13.0                     174.0  0.049366                    1.500000\n",
              "14.0                     118.0  0.057739                    1.500000\n",
              "15.0                      85.0  0.123585                    1.600000\n",
              "16.0                      73.0  0.113242                    1.369863\n",
              "17.0                      51.0  0.036044                    1.431373\n",
              "18.0                      66.0  0.039612                    1.378788\n",
              "19.0                      45.0  0.063808                    1.533333\n",
              "20.0                      31.0 -0.021053                    1.419355\n",
              "21.0                      31.0 -0.107834                    1.741935\n",
              "22.0                      27.0 -0.038961                    1.518519\n",
              "23.0                      10.0  0.037945                    1.800000\n",
              "24.0                      14.0 -0.010870                    1.428571\n",
              "25.0                      10.0  0.060667                    1.400000\n",
              "26.0                       4.0 -0.015385                    1.500000\n",
              "27.0                       9.0  0.205445                    1.888889\n",
              "28.0                      10.0 -0.062434                    2.000000\n",
              "29.0                      18.0 -0.008484                    1.888889\n",
              "30.0                      12.0 -0.039847                    1.500000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYFWdIOx6Sn1"
      },
      "source": [
        "with open('latex_table.txt', 'w') as f:\n",
        "    f.write(entity_graph_df_aggr.to_latex())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQljLZkzK0tR"
      },
      "source": [
        "from copy import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1X8sFDcqV5N"
      },
      "source": [
        "with open('sanity_check_data_entity_graph.json') as f:\n",
        "    sanity_check_data_entity_graph = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOWCUEZsNyqc",
        "outputId": "5155a732-47b2-4890-a3e4-f54dae8ca6cc"
      },
      "source": [
        "sanity_check_data_entity_graph.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['real', 'random'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD9Hzpmds7eY",
        "outputId": "7cbba98f-b078-4552-beef-ede8c6cab8c3"
      },
      "source": [
        "real_prediction =  [scoring_entity_graph_beam_search(json_graph.node_link_graph(sample[2]), sample[1], sample[3][0]) for sample in sanity_check_data_entity_graph['real']]\n",
        "random_prediction = [scoring_entity_graph_beam_search(json_graph.node_link_graph(sample[2]), sample[1], sample[3][0]) for sample in sanity_check_data_entity_graph['random']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjWCIZW5vTan",
        "outputId": "1f417f78-dcea-4366-99f6-f68e7fce44a0"
      },
      "source": [
        "real_prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'order': [4, 8, 5, 2, 6, 3, 7, 1, 0], 'score': -17.67351146782644},\n",
              " {'order': [0, 2, 3, 1], 'score': -7.718962635598061},\n",
              " {'order': [5, 0, 4, 1, 6, 3, 7, 9, 15, 11, 2, 14, 17, 13, 16, 10, 12, 8],\n",
              "  'score': -35.530161927976174},\n",
              " {'order': [7, 0, 3, 2, 6, 8, 5, 1, 9, 4], 'score': -19.238074785558357},\n",
              " {'order': [2, 1, 10, 3, 16, 6, 18, 8, 0, 5, 13, 9, 17, 12, 11, 7, 15, 4, 14],\n",
              "  'score': -36.96463414296116}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2CfGbi_vWcD",
        "outputId": "eb061a38-7425-4907-869a-164e1cdac3ce"
      },
      "source": [
        "from statistics import mean\n",
        "mean(sample['score'] for sample in real_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-23.425068991984038"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKUKPecBvf8l",
        "outputId": "dfc0a104-9eea-4c10-97dc-25901bc34e78"
      },
      "source": [
        "from statistics import mean\n",
        "mean(sample['score'] for sample in random_prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-21.887044893920315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}