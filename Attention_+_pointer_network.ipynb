{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention + pointer network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP8eq4rlgjGJgbfBv9y+FPH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizavetaNosova/Ordering-text-quest-fragments/blob/main/Attention_%2B_pointer_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Tb152tyZJj"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import json\n",
        "from networkx.readwrite import json_graph\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from gensim.models import FastText\n",
        "from nltk import wordpunct_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXYPEVdRXcI5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plRg-_qwUJ1m",
        "outputId": "7df1a0a1-185b-47c3-8c2c-2eda36d8e50d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "iW1y07hfNcjz"
      },
      "source": [
        "#@title Текст заголовка по умолчанию\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbgIezkvNlT5"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrZwMp3vNlVL"
      },
      "source": [
        "os.chdir('gdrive/MyDrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7Gd63q3anm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "310Yl1DjBfaf"
      },
      "source": [
        "fasttext_model = FastText.load_fasttext_format('cc.ru.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sT5kUzADMu0L",
        "outputId": "4adbde92-9a34-4892-bac5-36bd1a025153"
      },
      "source": [
        "fasttext_model['кошка'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbH2j4bmM6Ll",
        "outputId": "d76038a3-6fcb-4add-f898-a5a3b6ccd677"
      },
      "source": [
        "np.zeros(300).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK4SZqajDVyH"
      },
      "source": [
        "class BatchEmbedder:\n",
        "    def __init__(self, fasttext_model):\n",
        "        self.fasttext_model = fasttext_model\n",
        "    \n",
        "    def embed_tokens(self, tokens:list):\n",
        "        return [self.fasttext_model[token] for token in tokens if token in self.fasttext_model]\n",
        "    \n",
        "    def __call__(self, tokenized_texts):\n",
        "        max_len = max([len(tokenized_text) for tokenized_text in tokenized_texts])\n",
        "        embedded_texts = [self.embed_tokens(tokenized_text) for tokenized_text in tokenized_texts]\n",
        "        embedded_texts = [text for text in embedded_texts if text]\n",
        "        padded_embedded_tokens = [self.pad(embedded_text, max_len) for embedded_text in embedded_texts]\n",
        "        return torch.tensor(padded_embedded_tokens)\n",
        "        \n",
        "    def pad(self, embeddings:list, max_len:int):\n",
        "        sequence_beginning = embeddings[:max_len]\n",
        "        embedding_dim = len(embeddings[0])\n",
        "        pads = [np.zeros(embedding_dim) for i in range(max_len-len(sequence_beginning))]\n",
        "        return pads + sequence_beginning\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SapiQhRjSaBD"
      },
      "source": [
        "ORDERING_DATA_DIRECTORY = 'tokenized_ordering_train'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl-FFwGR9GwX"
      },
      "source": [
        "ORDERING_DATA_DIRECTORY_TEST = 'tokenized_ordering_test_joined'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qs8GNk-fSaC6"
      },
      "source": [
        "class QuestOrderDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, paths_directory, embedder):\n",
        "        super().__init__()\n",
        "        self.directory = paths_directory\n",
        "        self.files = os.listdir(paths_directory)\n",
        "        self.embedder=embedder\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = os.path.join(self.directory, self.files[idx])\n",
        "        _, order, graph_json_data, __ = json.load(open(file, encoding='utf-8'))\n",
        "        graph = json_graph.node_link_graph(graph_json_data)\n",
        "        nodes = graph.nodes()\n",
        "        tokenized_fragments = [self.get_tokenized_text(nodes[node]) for node in order]\n",
        "        return torch.squeeze(self.embedder(tokenized_fragments))\n",
        "\n",
        "\n",
        "\n",
        "    def get_tokenized_text(self, node):\n",
        "        if 'tokenized_text' in node:\n",
        "            return node['tokenized_text']\n",
        "        elif 'fragment_text' in node and isinstance(node['fragment_text'], str):\n",
        "            return wordpunct_tokenize(node['fragment_text'])\n",
        "        else:\n",
        "            return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XCccpTP2HvB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDnXYL-k1ELU"
      },
      "source": [
        "class SourceTextQuestOrderDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, paths_directory):\n",
        "        super().__init__()\n",
        "        self.directory = paths_directory\n",
        "        self.files = os.listdir(paths_directory)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file = os.path.join(self.directory, self.files[idx])\n",
        "        _, order, graph_json_data, __ = json.load(open(file, encoding='utf-8'))\n",
        "        graph = json_graph.node_link_graph(graph_json_data)\n",
        "        nodes = graph.nodes()\n",
        "        fragments = [self.get_text(nodes[node]) for node in order]\n",
        "        return fragments\n",
        "\n",
        "\n",
        "\n",
        "    def get_text(self, node):\n",
        "        if 'joined_fragment_text' in node and isinstance(node['joined_fragment_text'], str):\n",
        "            return node['joined_fragment_text']\n",
        "        elif  'fragment_text' in node and  isinstance(node['fragment_text'], str):\n",
        "            return node['fragment_text']\n",
        "        else:\n",
        "            return ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRlGLJkOtGBd"
      },
      "source": [
        "embedder = BatchEmbedder(fasttext_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZeksSsebuhD"
      },
      "source": [
        "train_dataset = QuestOrderDataset('tokenized_ordering_train', embedder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgLlNr-w9UD7"
      },
      "source": [
        "test_dataset =  QuestOrderDataset('tokenized_ordering_test', embedder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO7IAgge1ZlW"
      },
      "source": [
        "test_dataset_texts = SourceTextQuestOrderDataset(ORDERING_DATA_DIRECTORY_TEST)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0TpCR4_GlQ_"
      },
      "source": [
        "class DeepAttentiveOrderingNetwork(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim=300, encoder_lstm_hidden_dim=50, num_heads=5, attention_iterations=5, dropout=0.1, max_path_len=40, dim_feed_forward=500, pointer_embedding_dim=15):\n",
        "        super().__init__()\n",
        "        self.iterations = attention_iterations\n",
        "  \n",
        "        self.fragment_encoder_lstm = torch.nn.LSTM(embedding_dim, encoder_lstm_hidden_dim, bidirectional=True, batch_first=True)\n",
        "        lstm_encoder_output_dim = encoder_lstm_hidden_dim*4\n",
        "        self.paragraph_encoder_layer1 = torch.nn.TransformerEncoderLayer(d_model=lstm_encoder_output_dim, nhead=num_heads, dim_feedforward=dim_feed_forward)\n",
        "        self.paragraph_encoder_layer2 = torch.nn.TransformerEncoderLayer(d_model=lstm_encoder_output_dim, nhead=num_heads, dim_feedforward=dim_feed_forward)\n",
        "\n",
        "       #pointer module\n",
        "        self.history_lstm = torch.nn.LSTM(lstm_encoder_output_dim, lstm_encoder_output_dim, batch_first=True)\n",
        "        self.history_linear = torch.nn.Linear(lstm_encoder_output_dim, pointer_embedding_dim)\n",
        "        self.candidates_linear = torch.nn.Linear(lstm_encoder_output_dim, pointer_embedding_dim)\n",
        "        self.pointing_linear = torch.nn.Linear(pointer_embedding_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, fragments_in_correct_order):\n",
        "        fragments_representation = self.encode(fragments_in_correct_order)\n",
        "        global_representation = self.get_global_representation(fragments_representation)\n",
        "        history = torch.cat((torch.unsqueeze(global_representation, 0), fragments_representation.clone()))\n",
        "        history = torch.unsqueeze(history, 0)\n",
        "        scores = self.point(history, fragments_representation)\n",
        "        return scores\n",
        "  \n",
        "    def encode(self, embedded_fragments):\n",
        "        representation = self.lstm_encode(embedded_fragments)\n",
        "        representation = torch.unsqueeze(representation, 0)\n",
        "        for i in range(self.iterations-1):\n",
        "            representation = self.paragraph_encoder_layer1(representation)\n",
        "        representation = self.paragraph_encoder_layer2(representation)\n",
        "        return torch.squeeze(representation)\n",
        "\n",
        "    def lstm_encode(self, embeddings):\n",
        "        _, representation = self.fragment_encoder_lstm(embeddings.double())\n",
        "        representation_vector = torch.cat((representation[0].clone()[0].clone(), representation[0].clone()[1].clone(), representation[1].clone()[0].clone(), representation[1].clone()[1].clone()),1)\n",
        "        return torch.tanh(representation_vector)\n",
        "\n",
        "    def get_global_representation(self, fragments_representations):\n",
        "        return torch.mean(fragments_representations, 0)\n",
        "\n",
        "    def point(self, history, candidates, shuffle=True):\n",
        "        history, _ = self.history_lstm(history)\n",
        "        history = torch.tanh(torch.squeeze(self.history_linear(history)))\n",
        "        if shuffle:\n",
        "             candidates, new_order = self.controlled_shuffle(candidates)\n",
        "        else:\n",
        "            new_order = [i for i in range(candidates.shape[0])]\n",
        "        candidates = torch.tanh(self.candidates_linear(candidates))\n",
        "        scores = torch.zeros(history.shape[0]-1, candidates.shape[0])\n",
        "        for step_idx in range(history.shape[0]-1):\n",
        "            #history_step = history[step_idx, :].clone()\n",
        "            history_step = history[step_idx].clone()\n",
        "            for candidate_idx in range(candidates.shape[0]):\n",
        "                candidate = candidates[candidate_idx, :].clone()   \n",
        "                scores[step_idx, candidate_idx] = torch.squeeze(torch.tanh(self.pointing_linear(candidate+history_step)))\n",
        "        return torch.softmax(scores, 0), new_order\n",
        "\n",
        "    def controlled_shuffle(self, candidates_tensor):\n",
        "        ordering_list = [(i, random.random(), row) for i, row in enumerate(candidates_tensor)]\n",
        "        ordering_list = sorted(ordering_list, key=lambda x: x[1])\n",
        "        new_order = [row_data[0] for row_data in ordering_list]\n",
        "        new_candidates_tensor = torch.cat(tuple([torch.unsqueeze(row_data[2], 0) for row_data in ordering_list]), 0)\n",
        "        return new_candidates_tensor, new_order"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyyNmNZk-yzg"
      },
      "source": [
        "model = DeepAttentiveOrderingNetwork()\n",
        "model.double()\n",
        "\n",
        "\n",
        "model.train()\n",
        "optimizer =  torch.optim.Adam([{'params': model.parameters()}], \n",
        "                               lr = 1e-3)\n",
        "criterion = torch.nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u4fTK8Y4LM0",
        "outputId": "e5694876-e3dd-4d43-ae8f-c76a1a03cc14"
      },
      "source": [
        "losses = []\n",
        "for i in tqdm(range(len(train_dataset))):\n",
        "    path = train_dataset[i]\n",
        "    prediction, correct_order = model(path)\n",
        "    correct = torch.zeros(prediction.shape)\n",
        "    for i, position in enumerate(correct_order):\n",
        "        correct[position][i] += 1\n",
        "    loss = criterion(prediction, correct)\n",
        "    losses.append(float(loss))\n",
        "    loss.backward()\n",
        "    if len(losses)%10 == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    if len(losses)%150==0:\n",
        "        torch.save(model.state_dict(), 'ordering_attention_fix_activation.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/7283 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "100%|██████████| 7283/7283 [2:16:56<00:00,  1.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-hEvyMvsDVl",
        "outputId": "0cc75539-2593-4049-e910-9103b4fb2fc2"
      },
      "source": [
        "model.load_state_dict(torch.load('ordering_attention_fix_activation.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnPxnEhI7oyT"
      },
      "source": [
        "def predict_order_beam_search(test_sample, model, num_best_candidates=3):\n",
        "    model.eval()\n",
        "    encoded_sample = model.encode(test_sample)\n",
        "    history = torch.unsqueeze(torch.unsqueeze(model.get_global_representation(encoded_sample), 0), 0)\n",
        "    #history = torch.unsqueeze(model.get_global_representation(encoded_sample), 0)\n",
        "    predicted_orders = [{'history':history, 'predicted_order':[], 'probability':1}]\n",
        "    for i in range(test_sample.shape[0]-2):\n",
        "        for order_data in predicted_orders:\n",
        "            current_history = order_data['history']\n",
        "            order_candidate = order_data['predicted_order']\n",
        "            padded_current_history = torch.cat((current_history, torch.zeros(1, len(test_sample)-current_history.shape[0], current_history.shape[-1])),1)\n",
        "            scores, _ = model.point(padded_current_history, encoded_sample, shuffle=False)\n",
        "            #print(scores)\n",
        "            current_step_scores = scores[i]\n",
        "            for already_used_sent in order_candidate:\n",
        "                current_step_scores[already_used_sent] *= -math.inf\n",
        "            for i in range(min(num_best_candidates, len(test_sample)-len(order_candidate))):\n",
        "                best_candidate_idx = int(current_step_scores.argmax(-1))\n",
        "                order_data['predicted_order'].append(best_candidate_idx)\n",
        "\n",
        "                order_data['history'] = torch.cat((order_data['history'], torch.unsqueeze(torch.unsqueeze(encoded_sample[best_candidate_idx], 0), 0)),1)\n",
        "\n",
        "                order_data['probability'] *= float(current_step_scores[best_candidate_idx])\n",
        "                current_step_scores[best_candidate_idx] *= -math.inf\n",
        "\n",
        "        predicted_orders = sorted(predicted_orders, key=lambda x: x['probability'], reverse=True)[:num_best_candidates]\n",
        "    return predicted_orders[0]['predicted_order']   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAf1eN0UhWgo"
      },
      "source": [
        "def scoring_predict_order_beam_search(texts, model, num_best_candidates=3, epsilon=10**(-9)):\n",
        "    tokenized_texts = [wordpunct_tokenize(text) for text in texts]\n",
        "    test_sample = embedder(tokenized_texts)\n",
        "    model.eval()\n",
        "    encoded_sample = model.encode(test_sample)\n",
        "    history = torch.unsqueeze(torch.unsqueeze(model.get_global_representation(encoded_sample), 0), 0)\n",
        "    #history = torch.unsqueeze(model.get_global_representation(encoded_sample), 0)\n",
        "    predicted_orders = [{'history':history, 'predicted_order':[], 'probability':0}]\n",
        "    for i in range(test_sample.shape[0]-2):\n",
        "        for order_data in predicted_orders:\n",
        "            current_history = order_data['history']\n",
        "            order_candidate = order_data['predicted_order']\n",
        "            padded_current_history = torch.cat((current_history, torch.zeros(1, len(test_sample)-current_history.shape[0], current_history.shape[-1])),1)\n",
        "            scores, _ = model.point(padded_current_history, encoded_sample, shuffle=False)\n",
        "            #print(scores)\n",
        "            current_step_scores = scores[i]\n",
        "            for already_used_sent in order_candidate:\n",
        "                current_step_scores[already_used_sent] *= -math.inf\n",
        "            for i in range(min(num_best_candidates, len(test_sample)-len(order_candidate))):\n",
        "                best_candidate_idx = int(current_step_scores.argmax(-1))\n",
        "                order_data['predicted_order'].append(best_candidate_idx)\n",
        "\n",
        "                order_data['history'] = torch.cat((order_data['history'], torch.unsqueeze(torch.unsqueeze(encoded_sample[best_candidate_idx], 0), 0)),1)\n",
        "\n",
        "                score = float(current_step_scores[best_candidate_idx]) if float(current_step_scores[best_candidate_idx]) > 0 else epsilon\n",
        "                order_data['probability'] += float(current_step_scores[best_candidate_idx])\n",
        "                current_step_scores[best_candidate_idx] *= -math.inf\n",
        "\n",
        "        predicted_orders = sorted(predicted_orders, key=lambda x: x['probability'], reverse=True)[:num_best_candidates]\n",
        "    return {'order':predicted_orders[0]['predicted_order'], 'score':predicted_orders[0]['probability']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WycLopGClOz3"
      },
      "source": [
        "with open('sanity_check_data.json') as f:\n",
        "    sanity_check_data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xewWVcBlXfQ",
        "outputId": "fb16d407-be50-45af-f1b4-3cb1c6a1049a"
      },
      "source": [
        "real_quests_predictions = [scoring_predict_order_beam_search(path, model) for path in sanity_check_data['real']]\n",
        "random_quests_predictions = [scoring_predict_order_beam_search(path, model) for path in sanity_check_data['random']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1YovFh7sq_a",
        "outputId": "fda03273-d9e0-47fc-cdd2-4804b429f07d"
      },
      "source": [
        "real_quests_predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'order': [1, 3, 5, 4, 2, 0, 7, 6, 8], 'score': 0.8619930446147919},\n",
              " {'order': [0, 1, 3, 2], 'score': 1.166579708456993},\n",
              " {'order': [14, 10, 0, 4, 5, 6, 8, 7, 12, 9, 15, 16, 1, 2, 3, 11, 17, 13],\n",
              "  'score': 0.769480399787426},\n",
              " {'order': [6, 8, 9, 2, 3, 4, 7, 5, 0, 1], 'score': 0.8388739749789238},\n",
              " {'order': [6, 7, 10, 0, 2, 3, 13, 11, 9, 14, 15, 1, 12, 4, 16, 17, 5, 8, 18],\n",
              "  'score': 0.7643137294799089}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_IMTYIKssKi",
        "outputId": "d845291c-2993-4959-f919-8de4cbe06d36"
      },
      "source": [
        "from statistics import mean\n",
        "mean([sample['score'] for sample in real_quests_predictions])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8802481714636088"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcdGtjUss1DM",
        "outputId": "40fb7494-f346-47a9-9a2f-299767030d68"
      },
      "source": [
        "mean([sample['score'] for sample in random_quests_predictions])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8802481662482023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYYA4yGUHYVx",
        "outputId": "6215d037-6afa-4c3f-ebf1-dcd8b75f8d48"
      },
      "source": [
        "result = []\n",
        "for sample in tqdm(test_dataset):\n",
        "    shuffled_sample, correct_order = model.controlled_shuffle(sample)\n",
        "    prediction = predict_order_beam_search(sample, model)\n",
        "    result.append({'correct': correct_order, 'predicted': prediction})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3430 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n",
            "100%|██████████| 3430/3430 [4:57:58<00:00,  5.21s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ6zLFvXdQj0"
      },
      "source": [
        "with open('basic_transformer_ordering_new.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(result, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SlQQOZxk1uF"
      },
      "source": [
        "with open('basic_transformer_ordering_new.json') as f:\n",
        "    result = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1LsWyx0lmNP"
      },
      "source": [
        "def longest_correct_subsequence(predicted, correct):\n",
        "    correct_transitions = set([(item_from, item_to) for item_from, item_to in zip(correct[:-1], correct[1:])])\n",
        "    predicted_transitions = [(item_from, item_to) for item_from, item_to in zip(predicted[:-1], predicted[1:])]\n",
        "    predicted_transitions_are_correct = [transition in correct_transitions for transition in predicted_transitions]\n",
        "    \n",
        "    longest_correct_transitions_subsequence = 0\n",
        "    current_correct_transitions_subsequence = 0\n",
        "    #Add False as last item to include last real item checking into the loop\n",
        "    for predicted_transition_is_correct in predicted_transitions_are_correct + [False]:\n",
        "        if predicted_transition_is_correct:\n",
        "            current_correct_transitions_subsequence += 1\n",
        "        else:\n",
        "            if current_correct_transitions_subsequence > longest_correct_transitions_subsequence:\n",
        "                longest_correct_transitions_subsequence = current_correct_transitions_subsequence\n",
        "            current_correct_transitions_subsequence = 0\n",
        "    #return number of items in longest correct sequence (not number of transitions)\n",
        "    return longest_correct_transitions_subsequence + 1 if longest_correct_transitions_subsequence else 0  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOKkkrNWlwqm"
      },
      "source": [
        "from scipy.stats import kendalltau\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRCj6BJtlzoG"
      },
      "source": [
        "simple_transformer_pointer_df = pd.DataFrame(columns = [\"sequence length\", \"Kendall's  tau\", \"Longest correct subsequence\"])\n",
        "for sequence_data in result:\n",
        "    predicted = sequence_data['predicted']\n",
        "    correct = sequence_data['correct']\n",
        "    tau = kendalltau(predicted, correct).correlation\n",
        "    lcs = longest_correct_subsequence(predicted, correct)\n",
        "    simple_transformer_pointer_df.loc[len(simple_transformer_pointer_df)] = [len(correct), tau, lcs]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "2V3Xl4KQmMaQ",
        "outputId": "841b5ff5-a106-4fe2-8460-c21502baafa1"
      },
      "source": [
        "simple_transformer_pointer_df_aggr = simple_transformer_pointer_df[simple_transformer_pointer_df[\"sequence length\"]<=30].groupby(\"sequence length\").describe()[[(\"Kendall's  tau\", 'count'),  (\"Kendall's  tau\",  'mean'), ('Longest correct subsequence',  'mean')]]\n",
        "simple_transformer_pointer_df_aggr "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"2\" halign=\"left\">Kendall's  tau</th>\n",
              "      <th>Longest correct subsequence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>mean</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sequence length</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>98.0</td>\n",
              "      <td>0.027211</td>\n",
              "      <td>1.336735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5.0</th>\n",
              "      <td>97.0</td>\n",
              "      <td>-0.051546</td>\n",
              "      <td>1.144330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6.0</th>\n",
              "      <td>149.0</td>\n",
              "      <td>-0.022819</td>\n",
              "      <td>1.375839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7.0</th>\n",
              "      <td>151.0</td>\n",
              "      <td>-0.002208</td>\n",
              "      <td>1.317881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8.0</th>\n",
              "      <td>130.0</td>\n",
              "      <td>-0.001648</td>\n",
              "      <td>1.307692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9.0</th>\n",
              "      <td>192.0</td>\n",
              "      <td>0.039352</td>\n",
              "      <td>1.307292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.0</th>\n",
              "      <td>175.0</td>\n",
              "      <td>-0.005206</td>\n",
              "      <td>1.194286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11.0</th>\n",
              "      <td>225.0</td>\n",
              "      <td>0.015919</td>\n",
              "      <td>1.342222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12.0</th>\n",
              "      <td>226.0</td>\n",
              "      <td>-0.014079</td>\n",
              "      <td>1.283186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13.0</th>\n",
              "      <td>184.0</td>\n",
              "      <td>-0.004041</td>\n",
              "      <td>1.336957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14.0</th>\n",
              "      <td>129.0</td>\n",
              "      <td>0.006730</td>\n",
              "      <td>1.372093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.0</th>\n",
              "      <td>107.0</td>\n",
              "      <td>-0.010592</td>\n",
              "      <td>1.514019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16.0</th>\n",
              "      <td>88.0</td>\n",
              "      <td>-0.045076</td>\n",
              "      <td>1.352273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17.0</th>\n",
              "      <td>79.0</td>\n",
              "      <td>-0.008004</td>\n",
              "      <td>1.291139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18.0</th>\n",
              "      <td>96.0</td>\n",
              "      <td>0.007898</td>\n",
              "      <td>1.239583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19.0</th>\n",
              "      <td>92.0</td>\n",
              "      <td>0.017162</td>\n",
              "      <td>1.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.0</th>\n",
              "      <td>67.0</td>\n",
              "      <td>0.006913</td>\n",
              "      <td>1.238806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21.0</th>\n",
              "      <td>65.0</td>\n",
              "      <td>-0.034579</td>\n",
              "      <td>1.353846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22.0</th>\n",
              "      <td>77.0</td>\n",
              "      <td>0.006128</td>\n",
              "      <td>1.233766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23.0</th>\n",
              "      <td>57.0</td>\n",
              "      <td>-0.010610</td>\n",
              "      <td>1.157895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24.0</th>\n",
              "      <td>61.0</td>\n",
              "      <td>-0.024828</td>\n",
              "      <td>1.327869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25.0</th>\n",
              "      <td>59.0</td>\n",
              "      <td>0.005537</td>\n",
              "      <td>1.305085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26.0</th>\n",
              "      <td>40.0</td>\n",
              "      <td>-0.031692</td>\n",
              "      <td>1.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27.0</th>\n",
              "      <td>63.0</td>\n",
              "      <td>0.011984</td>\n",
              "      <td>1.079365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28.0</th>\n",
              "      <td>41.0</td>\n",
              "      <td>0.037166</td>\n",
              "      <td>1.414634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29.0</th>\n",
              "      <td>69.0</td>\n",
              "      <td>-0.001428</td>\n",
              "      <td>1.275362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30.0</th>\n",
              "      <td>49.0</td>\n",
              "      <td>-0.022003</td>\n",
              "      <td>1.448980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Kendall's  tau           Longest correct subsequence\n",
              "                         count      mean                        mean\n",
              "sequence length                                                     \n",
              "4.0                       98.0  0.027211                    1.336735\n",
              "5.0                       97.0 -0.051546                    1.144330\n",
              "6.0                      149.0 -0.022819                    1.375839\n",
              "7.0                      151.0 -0.002208                    1.317881\n",
              "8.0                      130.0 -0.001648                    1.307692\n",
              "9.0                      192.0  0.039352                    1.307292\n",
              "10.0                     175.0 -0.005206                    1.194286\n",
              "11.0                     225.0  0.015919                    1.342222\n",
              "12.0                     226.0 -0.014079                    1.283186\n",
              "13.0                     184.0 -0.004041                    1.336957\n",
              "14.0                     129.0  0.006730                    1.372093\n",
              "15.0                     107.0 -0.010592                    1.514019\n",
              "16.0                      88.0 -0.045076                    1.352273\n",
              "17.0                      79.0 -0.008004                    1.291139\n",
              "18.0                      96.0  0.007898                    1.239583\n",
              "19.0                      92.0  0.017162                    1.250000\n",
              "20.0                      67.0  0.006913                    1.238806\n",
              "21.0                      65.0 -0.034579                    1.353846\n",
              "22.0                      77.0  0.006128                    1.233766\n",
              "23.0                      57.0 -0.010610                    1.157895\n",
              "24.0                      61.0 -0.024828                    1.327869\n",
              "25.0                      59.0  0.005537                    1.305085\n",
              "26.0                      40.0 -0.031692                    1.350000\n",
              "27.0                      63.0  0.011984                    1.079365\n",
              "28.0                      41.0  0.037166                    1.414634\n",
              "29.0                      69.0 -0.001428                    1.275362\n",
              "30.0                      49.0 -0.022003                    1.448980"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vCaH3wymn86"
      },
      "source": [
        "with open('latex_draft.txt', 'w') as f:\n",
        "    f.write(simple_transformer_pointer_df_aggr.to_latex())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}